title: NPFL122, Lecture 6
class: title, langtech, cc-by-nc-sa
# Rainbow, Policy Gradient Methods

## Milan Straka

### November 19, 2018

---
section: Refresh
# Function Approximation

We will approximate value function $v$ and/or state-value function $q$, choosing
from a family of functions parametrized by a weight vector $‚Üíw‚àà‚Ñù^d$.

We denote the approximations as
$$\begin{gathered}
  \hat v(s, ‚Üíw),\\
  \hat q(s, a, ‚Üíw).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(‚Üíw) ‚âù ‚àë_{s‚ààùì¢} Œº(s) \left[v_œÄ(s) - \hat v(s, ‚Üíw)\right]^2,$$
where the state distribution $Œº(s)$ is usually on-policy distribution.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $‚Üíw$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  ‚Üíw_{t+1} &‚Üê ‚Üíw_t - \frac{1}{2} Œ± ‚àá \left[v_œÄ(S_t) - \hat v(S_t, ‚Üíw_t)\right]^2\\
           &‚Üê ‚Üíw_t - Œ±\left[v_œÄ(S_t) - \hat v(S_t, ‚Üíw_t)\right] ‚àá \hat v(S_t, ‚Üíw_t).\\
\end{aligned}$$

As usual, the $v_œÄ(S_t)$ is estimated by a suitable sample. For example in Monte
Carlo methods, we use episodic return $G_t$, and in temporal difference methods,
we employ bootstrapping and use $R_{t+1} + Œ≥\hat v(S_{t+1}, ‚Üíw).$

---
section: DQN
# Deep Q Network

Off-policy Q-learning algorithm with a convolutional neural network function
approximation of action-value function.

Training can be extremely brittle (and can even diverge as shown earlier).

![w=65%,h=center](../05/dqn_architecture.pdf)

---
# Deep Q Networks

- Preprocessing: $210√ó160$ 128-color images are converted to grayscale and
  then resized to $84√ó84$.
~~~
- Frame skipping technique is used, i.e., only every $4^\textrm{th}$ frame
  (out of 60 per second) is considered, and the selected action is repeated on
  the other frames.
~~~
- Input to the network are last $4$ frames (considering only the frames kept by
  frame skipping), i.e., an image with $4$ channels.
~~~
- The network is fairly standard, performing
  - 32 filters of size $8√ó8$ with stride 4 and ReLU,
  - 64 filters of size $4√ó4$ with stride 2 and ReLU,
  - 64 filters of size $3√ó3$ with stride 1 and ReLU,
  - fully connected layer with 512 units and ReLU,
  - output layer with 18 output units (one for each action)

---
# Deep Q Networks

- Network is trained with RMSProp to minimize the following loss:
  $$ùìõ ‚âù ùîº_{(s, a, r, s')‚àº\mathit{data}}\left[(r + Œ≥ \max_{a'} Q(s', a'; \bar Œ∏) - Q(s, a; Œ∏))^2\right].$$
~~~
- An $Œµ$-greedy behavior policy is utilized.

~~~
Important improvements:
~~~
- experience replay: the generated episodes are stored in a buffer as $(s, a, r,
  s')$ quadruples, and for training a transition is sampled uniformly;
~~~
- separate target network $\bar Œ∏$: to prevent instabilities, a separate target
  network is used to estimate state-value function. The weights are not trained,
  but copied from the trained network once in a while;
~~~
- reward clipping of $(r + Œ≥ \max_{a'} Q(s', a'; \bar Œ∏) - Q(s, a; Œ∏))$ to $[-1, 1]$.

---
class: tablefull
# Deep Q Networks Hyperparameters

| Hyperparameter | Value |
|----------------|-------|
| minibatch size | 32 |
| replay buffer size | 1M |
| target network update frequency | 10k |
| discount factor | 0.99 |
| training frames | 50M |
| RMSProp learning rate and momentum | 0.00025, 0.95 |
| initial $Œµ$, final $Œµ$ and frame of final $Œµ$ | 1.0, 0.1, 1M |
| replay start size | 50k |
| no-op max | 30 |

---
section: Rainbow
# Rainbow

There have been many suggested improvements to the DQN architecture. In the end
of 2017, the _Rainbow: Combining Improvements in Deep Reinforcement Learning_
paper combines 7 of them into a single architecture they call _Rainbow_.

~~~
![w=40%,h=center](rainbow_results.pdf)

---
# Rainbow DQN Extensions

## Multi-step Learning

Instead of Q-learning, we use $n$-step variant Q-learning (to be exact, we use
$n$-step Expected Sarsa).

~~~
## Double Q-learning

Similarly to double Q-learning, instead of
$$r + Œ≥ \max_{a'} Q(s', a'; \bar Œ∏) - Q(s, a; Œ∏),$$
we minimize
$$r + Œ≥ Q(s', \argmax_{a'}Q(s', a'; Œ∏); \bar Œ∏) - Q(s, a; Œ∏).$$

---
# Rainbow DQN Extensions

## Prioritized Replay

Instead of sampling the transitions uniformly from the replay buffer,
we instead prefer those with a large TD error. Therefore, we sample transitions
according to their probability
$$p_t ‚àù |r + Œ≥ \max_{a'} Q(s', a'; \bar Œ∏) - Q(s, a; Œ∏)|^œâ,$$
where $œâ$ controls the shape of the distribution (which is uniform for $œâ=0$
and corresponds to TD error for $œâ=1$).

~~~
New transitions are inserted into the replay buffer with maximum probability
to support exploration of all encountered transitions.

---
# Rainbow DQN Extensions

## Duelling Networks

Instead of computing directly $Q(s, a; Œ∏)$, we compose it from the following quantities:
- value function for a given state $s$,
- advantage function computing an _advantage_ of using action $a$ in state $s$.

$$Q(s, a) ‚âù V(f(s; Œ∂); Œ∑) + A(f(s; Œ∂), a; œà) + \frac{\sum_{a' ‚àà ùìê} A(f(s; Œ∂), a'; œà)}{|ùìê|}$$

![w=25%,h=center](dqn_dueling_architecture.pdf)

---
# Rainbow DQN Extensions

## Duelling Networks

![w=32%,h=center](dqn_dueling_visualization.pdf)

---
# Rainbow DQN Extensions

## Distributional RL

---
# Rainbow DQN Extensions

## Noisy Nets

---
# Rainbow Ablations

![w=90%,h=center](rainbow_ablations.pdf)

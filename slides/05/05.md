title: NPFL122, Lecture 5
class: title, langtech, cc-by-nc-sa
# Function Approximation,<br>Eligibility Traces

## Milan Straka

### November 12, 2018

---
section: Refresh
# $n$-step Methods

![w=40%,f=right](../04/nstep_td.pdf)

Full return is
$$G_t = ∑_{k=t}^∞ R_{k+1},$$
one-step return is
$$G_{t:t+1} = R_{t+1} + γ V_t(S_{t+1}).$$

We can generalize both into $n$-step returns:
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n V_{t+n-1}(S_{t+n}).$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$.

---
# $n$-step Sarsa

![w=30%,f=right](../04/nstep_td.pdf)

Defining the $n$-step return to utilize action-value function as
$$G_{t:t+n} ≝ \left(∑_{k=t}^{t+n-1} γ^{k-t} R_{k+1}\right) + γ^n Q_{t+n-1}(S_{t+n}, A_{t+n})$$
with $G_{t:t+n} ≝ G_t$ if $t+n ≥ T$, we get the following straightforward
update rule:
$$Q_{t+n}(S_t, A_t) ≝ Q_{t+n-1}(S_t, A_t) + α\left[G_{t:t+n} - Q_{t+n-1}(S_t, A_t)\right].$$

![w=55%,h=center](../04/nstep_sarsa_example.pdf)

---
# Off-policy $n$-step Without Importance Sampling

![w=10%,f=right](../04/tree_backup_example.pdf)

We now derive the $n$-step reward, starting from one-step:
$$G_{t:t+1} ≝ R_{t+1} + ∑\nolimits_a π(a|S_{t+1}) Q_{t}(S_{t+1}, a).$$

~~~
For two-step, we get:
$$G_{t:t+2} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q_{t}(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+2}.$$

~~~
Therefore, we can generalize to:
$$G_{t:t+n} ≝ R_{t+1} + γ∑\nolimits_{a≠A_{t+1}} π(a|S_{t+1}) Q_{t}(S_{t+1}, a) + γπ(A_{t+1}|S_{t+1})G_{t+1:t+n}.$$

---
# Function Approximation

We will approximate value function $v$ and/or state-value function $q$, choosing
from a family of functions parametrized by a weight vector $→w∈ℝ^d$.

We denote the approximations as
$$\begin{gathered}
  \hat v(s, →w),\\
  \hat q(s, a, →w).
\end{gathered}$$

~~~
We utilize the _Mean Squared Value Error_ objective, denoted $\overline{VE}$:
$$\overline{VE}(→w) ≝ ∑_{s∈𝓢} μ(s) \left[v_π(s) - \hat v(s, →w)\right]^2,$$
where the state distribution $μ(s)$ is usually on-policy distribution.

---
# Gradient and Semi-Gradient Methods

The functional approximation (i.e., the weight vector $→w$) is usually optimized
using gradient methods, for example as
$$\begin{aligned}
  →w_{t+1} &← →w_t - \frac{1}{2} α ∇ \left[v_π(S_t) - \hat v(S_t, →w_t)\right]^2\\
           &← →w_t - α\left[v_π(S_t) - \hat v(S_t, →w_t)\right] ∇ \hat v(S_t, →w_t).\\
\end{aligned}$$

As usual, the $v_π(S_t)$ is estimated by a suitable sample. For example in Monte
Carlo methods, we use episodic return $G_t$, and in temporal difference methods,
we employ bootstrapping and use $R_{t+1} + γ\hat v(S_{t+1}, →w).$

---
# Linear Methods

A simple special case of function approximation are linear methods, where
$$\hat v(→x(s), →w) ≝ →x(s)^T →w = ∑x(s)_i w_i.$$

The $→x(s)$ is a representation of state $s$, which is a vector of the same size
as $→w$. It is sometimes called a _feature vector_.

The SGD update rule then becomes
$$→w_{t+1} ← →w_t - α\left[v_π(S_t) - \hat v(→x(S_t), →w_t)\right] →x(S_t).$$

---
# Feature Construction for Linear Methods

Many methods developed in the past:

- state aggregation,

- polynomials

- Fourier basis

- tile coding

- radial basis functions

But of course, nowadays we use deep neural networks which construct a suitable
feature vector automatically as a latent variable (the last hidden layer).

---
section: Tile Coding
# Tile Coding

![w=100%,mh=90%,v=middle](../04/tile_coding.pdf)

If $t$ overlapping tiles are used, the learning rate is usually normalized as $α/t$.

---
# Tile Coding

For example, on the 1000-state random walk example, the performance of tile
coding surpasses state aggregation:

![w=60%,h=center](../04/tile_coding_performance.pdf)

---
# Asymmetrical Tile Coding

In higher dimensions, the tiles should have asymmetrical offsets, with
a sequence of $(1, 3, 5, …, 2d-1)$ being a good choice.

![w=50%,h=center](../04/tile_coding_asymmetrical.pdf)

---
section: Semi-Gradient TD
# Temporal Difference Semi-Gradient Policy Evaluation

In TD methods, we again use bootstrapping to estimate
$v_π(S_t)$ as $R_{t+1} + γ\hat v(S_{t+1}, →w).$

~~~
![w=70%,h=center](grad_td_estimation.pdf)

~~~
Note that such algorithm is called _semi-gradient_, because it does not
backpropagate through $\hat v(S', →w)$.

---
# Temporal Difference Semi-Gradient Policy Evaluation

An important fact is that linear semi-gradient TD methods do not converge to
$\overline{VE}$. Instead, they converge to a different _TD fixed point_
$→w_\mathrm{TD}$.

~~~
It can be proven that
$$\overline{VE}(→w_\mathrm{TD}) ≤ \frac{1}{1-γ} \min_→w \overline{VE}(→w).$$

~~~
However, when $γ$ is close to one, the multiplication factor in the above bound
is quite large.

---
# Temporal Difference Semi-Gradient Policy Evaluation

As before, we can utilize $n$-step TD methods.

![w=60%,h=center](grad_td_nstep_estimation.pdf)

---
# Temporal Difference Semi-Gradient Policy Evaluation

![w=100%,v=middle](grad_td_estimation_example.pdf)

---
# Sarsa with Function Approximation

Until now, we talked only about policy evaluation. Naturally, we can extend it
to a full Sarsa algorithm:

![w=80%,h=center](grad_sarsa.pdf)

---
# Sarsa with Function Approximation

Additionally, we can incorporate $n$-step returns:

![w=55%,h=center](grad_sarsa_nstep.pdf)

---
# Mountain Car Example

![w=65%,h=center](mountain_car.pdf)

The performances are for semi-gradient Sarsa($λ$) algorithm (which we did not
talked about yet) with tile coding of 8 overlapping tiles covering position and
velocity, with offsets of $(1, 3)$.

---
# Mountain Car Example

![w=50%,h=center](mountain_car_performance_1and8_step.pdf)
![w=50%,h=center](mountain_car_performance_nstep.pdf)

title: NPFL122, Lecture 7
class: title, langtech, cc-by-nc-sa
# Policy Gradient Methods

## Milan Straka

### November 26, 2018

---
section: Policy Gradient Methods
# Policy Gradient Methods

Instead of predicting expected returns, we could train the method to directly
predict the policy
$$π(a | s; →θ).$$

~~~
Obtaining the full distribution over all actions would also allow us to sample
the actions according to the distribution $π$ instead of just $ε$-greedy
sampling.

~~~
However, to train the network, we maximize the expected return $v_π(s)$ and to
that account we need to compute its _gradient_ $∇_→θ v_π(s)$.

---
# Policy Gradient Theorem

Let $π(a | s; →θ)$ be a parametrized policy. We denote the initial state
distribution as $h(s)$ and the on-policy distribution under $π$ as $μ(s)$.
Let also $J(→θ) ≝ 𝔼_{h, π} v_π(s)$.

~~~
Then
$$∇_→θ v_π(s) ∝ ∑_{s'∈𝓢} P(s → s') ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ)$$
and
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ).$$

---
# Proof of Policy Gradient Theorem

$\displaystyle ∇v_π(s) = ∇ \Big[ ∑\nolimits_a π(a|s; →θ) q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ q_π(s, a) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) ∇ \big(∑\nolimits_{s'} p(s'|s, a)(r + v_π(s'))\big) \Big]$

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \big(∑\nolimits_{s'} p(s'|s, a) ∇ v_π(s')\big) \Big]$

~~~
_We now expand $v_π(s')$._

~~~
$\displaystyle \phantom{∇v_π(s)} = ∑\nolimits_a \Big[ ∇ π(a|s; →θ) q_π(s, a) + π(a|s; →θ) \Big(∑\nolimits_{s'} p(s'|s, a)\Big(\\
          \qquad\qquad\qquad\qquad ∑\nolimits_{a'} \Big[ ∇ π(a'|s'; →θ) q_π(s', a') + π(a'|s'; →θ) \Big(∑\nolimits_{s''} p(s''|s', a')\Big) \big) \Big]$

~~~
_Continuing to expand all $v_π(s')$, we obtain the following:_

$\displaystyle ∇v_π(s) = ∑_{s'∈𝓢} P(s → s') ∑_{a ∈ 𝓐} q_π(s', a) ∇_→θ π(a | s'; →θ).$
---
# Policy Gradient Methods

In addition to discarding $ε$-greedy action selection, policy gradient methods
allow producing policies which are by nature stochastic, as in card games with
imperfect information, while the action-value methods have no natural way of
finding stochastic policies (distributional RL might be of some use though).

~~~
![w=75%,h=center](stochastic_policy_example.pdf)

---
# REINFORCE Algorithm

The REINFORCE algorithm (Williams, 1992) uses directly the policy gradient
theorem, maximizing $J(→θ) ≝ 𝔼_{h, π} v_π(s)$. To compute the gradient
$$∇_→θ J(→θ) ∝ ∑_{s∈𝓢} μ(s) ∑_{a ∈ 𝓐} q_π(s, a) ∇_→θ π(a | s; →θ),$$
REINFORCE algorithm estimates the $q_π(s, a)$ by a single sample.

~~~
![w=75%,h=center](reinforce.pdf)
